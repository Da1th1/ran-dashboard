{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornerstone A&D Design Data Cleaning and Analysis\n",
    "\n",
    "This notebook provides an interactive environment for cleaning and analyzing the Cornerstone A&D Design Master Tracker data. It includes sections for:\n",
    "\n",
    "1. Data Loading and Initial Inspection\n",
    "2. Missing Value Analysis\n",
    "3. Date Validation and Cleaning\n",
    "4. Duplicate Detection\n",
    "5. Data Type Validation\n",
    "6. Value Validation\n",
    "7. Data Export\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect data\n",
    "df = pd.read_csv('Cornerstone - A&D - Design Master Tracker.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_inspect_data():\n",
    "    \"\"\"Load and display initial information about the dataset.\"\"\"\n",
    "    # Load the main dataset\n",
    "    main_df = pd.read_csv('Cornerstone - A&D - Design Master Tracker.csv', low_memory=False)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(\"Dataset Shape:\", main_df.shape)\n",
    "    print(\"\\nDataset Info:\")\n",
    "    main_df.info()\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(main_df.head())\n",
    "    \n",
    "    return main_df\n",
    "\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"Analyze and visualize missing values in the dataset.\"\"\"\n",
    "    # Calculate missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentages = (missing_values / len(df) * 100).round(2)\n",
    "    \n",
    "    # Create a DataFrame with missing value information\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Missing Percentage': missing_percentages\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    # Display missing value information\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Create a bar plot of missing values\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.barplot(x=missing_df.index, y='Missing Percentage', data=missing_df)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Missing Values by Column (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def validate_dates(df, date_col):\n",
    "    \"\"\"Validate dates in a specific column.\"\"\"\n",
    "    try:\n",
    "        # Convert to datetime\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        \n",
    "        # Check for future dates\n",
    "        future_dates = df[df[date_col] > pd.Timestamp.now()]\n",
    "        \n",
    "        # Check for old dates (before 2020)\n",
    "        old_dates = df[df[date_col] < pd.Timestamp('2020-01-01')]\n",
    "        \n",
    "        # Check for invalid dates (1900-01-01)\n",
    "        invalid_dates = df[df[date_col] == pd.Timestamp('1900-01-01')]\n",
    "        \n",
    "        return {\n",
    "            'future_dates': len(future_dates),\n",
    "            'old_dates': len(old_dates),\n",
    "            'invalid_dates': len(invalid_dates)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def analyze_dates(df):\n",
    "    \"\"\"Analyze all date columns in the dataset.\"\"\"\n",
    "    # Identify date columns\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower() or 'issued' in col.lower() or 'approved' in col.lower()]\n",
    "    \n",
    "    print(\"Potential date columns:\")\n",
    "    print(date_columns)\n",
    "    \n",
    "    # Validate each date column\n",
    "    date_validation = {}\n",
    "    for col in date_columns:\n",
    "        date_validation[col] = validate_dates(df, col)\n",
    "    \n",
    "    # Display validation results\n",
    "    print(\"\\nDate validation results:\")\n",
    "    for col, results in date_validation.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"Check for duplicate rows and Site IDs.\"\"\"\n",
    "    # Check for duplicate rows\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "    \n",
    "    if duplicate_rows > 0:\n",
    "        print(\"\\nDuplicate rows:\")\n",
    "        print(df[df.duplicated(keep=False)].sort_values(df.columns[0]))\n",
    "    \n",
    "    # Check for duplicate Site IDs\n",
    "    if 'Site ID' in df.columns:\n",
    "        duplicate_sites = df['Site ID'].duplicated().sum()\n",
    "        print(f\"\\nNumber of duplicate Site IDs: {duplicate_sites}\")\n",
    "        \n",
    "        if duplicate_sites > 0:\n",
    "            print(\"\\nDuplicate Site IDs:\")\n",
    "            print(df[df['Site ID'].duplicated(keep=False)].sort_values('Site ID'))\n",
    "\n",
    "def validate_data_types(df):\n",
    "    \"\"\"Validate data types and check for potential issues.\"\"\"\n",
    "    # Display data types\n",
    "    print(\"Data types of all columns:\")\n",
    "    print(pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data Type': df.dtypes\n",
    "    }))\n",
    "    \n",
    "    # Check for potential type mismatches\n",
    "    potential_issues = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Check numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_numeric = df[pd.to_numeric(df[col], errors='coerce').isna() & df[col].notna()]\n",
    "            if not non_numeric.empty:\n",
    "                potential_issues.append(f\"Column '{col}' contains non-numeric values\")\n",
    "        \n",
    "        # Check date columns\n",
    "        if pd.api.types.is_datetime64_dtype(df[col]):\n",
    "            non_date = df[pd.to_datetime(df[col], errors='coerce').isna() & df[col].notna()]\n",
    "            if not non_date.empty:\n",
    "                potential_issues.append(f\"Column '{col}' contains non-date values\")\n",
    "    \n",
    "    if potential_issues:\n",
    "        print(\"\\nPotential data type issues found:\")\n",
    "        for issue in potential_issues:\n",
    "            print(f\"- {issue}\")\n",
    "    else:\n",
    "        print(\"\\nNo data type issues found.\")\n",
    "\n",
    "def validate_values(df):\n",
    "    \"\"\"Validate values in key columns.\"\"\"\n",
    "    # Check unique values in key columns\n",
    "    key_columns = ['NS Status', 'Site Type', 'Client Priority']\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\nUnique values in '{col}':\")\n",
    "            print(df[col].dropna().unique())\n",
    "    \n",
    "    # Check for inconsistent dates\n",
    "    if all(col in df.columns for col in ['DD\\n(F)', 'DD Issued Client (A)']):\n",
    "        inconsistent_dates = df[\n",
    "            (df['DD Issued Client (A)'].notna()) & \n",
    "            (df['DD\\n(F)'].notna()) & \n",
    "            (df['DD Issued Client (A)'] < df['DD\\n(F)'])\n",
    "        ]\n",
    "        \n",
    "        if not inconsistent_dates.empty:\n",
    "            print(f\"\\nFound {len(inconsistent_dates)} projects where DD was issued before the forecast date\")\n",
    "            print(inconsistent_dates)\n",
    "        else:\n",
    "            print(\"\\nNo inconsistent dates found between DD forecast and actual dates\")\n",
    "\n",
    "def clean_and_export_data(df):\n",
    "    \"\"\"Clean the dataset and export it to CSV.\"\"\"\n",
    "    # Create a cleaned version of the dataset\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Add your cleaning steps here\n",
    "    # For example:\n",
    "    # - Convert dates to proper format\n",
    "    # - Handle missing values\n",
    "    # - Remove duplicates\n",
    "    # - Fix data type issues\n",
    "    \n",
    "    # Export the cleaned dataset\n",
    "    cleaned_df.to_csv('cleaned_cornerstone_data.csv', index=False)\n",
    "    print(\"Cleaned dataset exported to 'cleaned_cornerstone_data.csv'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all data cleaning and analysis steps.\"\"\"\n",
    "    # Load and inspect data\n",
    "    print(\"Loading and inspecting data...\")\n",
    "    df = load_and_inspect_data()\n",
    "    \n",
    "    # Analyze missing values\n",
    "    print(\"\\nAnalyzing missing values...\")\n",
    "    analyze_missing_values(df)\n",
    "    \n",
    "    # Analyze dates\n",
    "    print(\"\\nAnalyzing dates...\")\n",
    "    analyze_dates(df)\n",
    "    \n",
    "    # Check duplicates\n",
    "    print(\"\\nChecking for duplicates...\")\n",
    "    check_duplicates(df)\n",
    "    \n",
    "    # Validate data types\n",
    "    print(\"\\nValidating data types...\")\n",
    "    validate_data_types(df)\n",
    "    \n",
    "    # Validate values\n",
    "    print(\"\\nValidating values...\")\n",
    "    validate_values(df)\n",
    "    \n",
    "    # Clean and export data\n",
    "    print(\"\\nCleaning and exporting data...\")\n",
    "    clean_and_export_data(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
